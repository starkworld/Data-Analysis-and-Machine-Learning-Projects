{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bike sharing Patterns.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OnOhall-eoA",
        "colab_type": "text"
      },
      "source": [
        "## Predicting Bike Sharing Patterns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBnt08YT-nNe",
        "colab_type": "text"
      },
      "source": [
        "#### Importing libraries and dataset\n",
        "  Here dataset is in CSV file so we use read csv function to import it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tEZC8ahXzWh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Giva2nObZAOg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1 = pd.read_csv('hour.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u097uZ6G-8hp",
        "colab_type": "text"
      },
      "source": [
        "Describing the dataframe to see how values are distributed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bg2RvrQZQmC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1.describe"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tk8KlBL4_H2U",
        "colab_type": "text"
      },
      "source": [
        "## Data Exploratory Analysis "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h79sLwb_M16",
        "colab_type": "text"
      },
      "source": [
        "#### Plotting the features with dependent variable to describe the correlations between the perticular columns on dependent variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lqAVklRH_bl-",
        "colab_type": "text"
      },
      "source": [
        "This plot describe the correaltion between the individual month to bike count. It shows at third and ninth month the bike count is very high and in first and second month the count is very low"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8RYs66dz85q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "X = df1['mnth']\n",
        "Y = df1['cnt']\n",
        "plt.scatter(X,Y, color='green')\n",
        "plt.gca().set_ylabel('Count')\n",
        "plt.gca().set_xlabel('Months')\n",
        "\n",
        "plt.figure()\n",
        "X = df1['weathersit']\n",
        "Y = df1['cnt']\n",
        "plt.scatter(X,Y, color='orange')\n",
        "plt.gca().set_ylabel('Count')\n",
        "plt.gca().set_xlabel('weathersit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzRzgLgzAGcR",
        "colab_type": "text"
      },
      "source": [
        "This scatter plots describe the correlations between seasons and bike count in 3rd season the count was very high while 1st season is low. \n",
        "Other scatter plot describe the correlation between the hourly bike rides with total number of rides"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjQ5hrJo032D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "Y = df1['cnt']\n",
        "X = df1['season']\n",
        "_ = plt.scatter(X,Y, color='blue')\n",
        "plt.gca().set_ylabel('Count')\n",
        "plt.gca().set_xlabel('season')\n",
        "\n",
        "\n",
        "plt.figure()\n",
        "Y = df1['cnt']\n",
        "X = df1['hr']\n",
        "_ = plt.scatter(X,Y, color='black')\n",
        "plt.gca().set_ylabel('Count')\n",
        "plt.gca().set_xlabel('hr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsUl3opuAVhP",
        "colab_type": "text"
      },
      "source": [
        "This scatter plot describe the correlation between the holiday and weekday with bike count \n",
        "0 - Not holiday\n",
        "1 - holiday\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bg7pq4FV2DLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax1 = df1.plot(kind='scatter', x='holiday', y='cnt', color='r', label='Holiday')    \n",
        "ax2 = df1.plot(kind='scatter', x='weekday', y='cnt', color='g', label='Weekday', ax=ax1)    \n",
        "#ax3 = df.plot(kind='scatter', x='workingday', y='cnt', color='b', label='Working day', ax=ax1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbM51GJdAiGp",
        "colab_type": "text"
      },
      "source": [
        "This describe the corelation between the bike count and working day how it is effected. 1-means working day 0- means non working day\n",
        "this shows that during the working day the people are tended to use bikes more often"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdM3D40a30CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ax3 = df1.plot(kind='scatter', x='workingday', y='cnt', color='b', label='Working day')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJXmM2POC1hD",
        "colab_type": "text"
      },
      "source": [
        "Below is a plot showing the number of bike riders over the first 10 days or so in the data set. (Some days don't have exactly 24 entries in the data set, so it's not exactly 10 days.) You can see the hourly rentals here. This data is pretty complicated! The weekends have lower over all ridership and there are spikes when people are biking to and from work during the week. Looking at the data above, we also have information about temperature, humidity, and windspeed, all of these likely affecting the number of riders. You'll be trying to capture all this with your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXjdWp5Cq0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df1[:48*15].plot(x='dteday', y='cnt', color='red')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLS4NdHTVnHS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "casual_resgistered=pd.concat([df1.casual[:48*15], df1.registered[:48*15]], axis='columns')\n",
        "# Create subplots from 'annual'\n",
        "casual_resgistered.plot(subplots=False)\n",
        "# Display the subplots\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihewS6vsCUOG",
        "colab_type": "text"
      },
      "source": [
        "## Scaling the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8tV427qEl_s",
        "colab_type": "text"
      },
      "source": [
        "Here we have some categorical variables like season, weather, month. To include these in our model, we'll need to make binary dummy variables. This is simple to do with Pandas thanks to get_dummies().\n",
        "\n",
        "By plotting above we have seen that some labels are highly corelated and some are less corelated for better results we have to rule out the high and low corelated labels and we have to keep only moderatly corealted labels for better accuracy. \n",
        "So we can drop the labels which are high and least corelated to dependent variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpBdm8Fj5y7S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_var = ['season', 'weathersit', 'mnth', 'hr', 'weekday']\n",
        "for each in dummy_var:\n",
        "    dummies = pd.get_dummies(df1[each], prefix=each, drop_first=False)\n",
        "    df1 = pd.concat([df1, dummies], axis=1)\n",
        "\n",
        "fields_to_drop = ['instant', 'dteday', 'season', 'weathersit', \n",
        "                  'weekday', 'atemp', 'mnth', 'workingday', 'hr']\n",
        "clean_df = df1.drop(fields_to_drop, axis=1)\n",
        "clean_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBCq044oFi-_",
        "colab_type": "text"
      },
      "source": [
        "To make training the network easier, we have to standardize each of the continuous variables. That is, we'll shift and scale the variables such that they have zero mean and a standard deviation of 1.\n",
        "The scaling factors are saved so we can go backwards when we use the network for predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcS_kIZcmric",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "quant_features = ['casual', 'registered', 'cnt', 'temp', 'hum', 'windspeed']\n",
        "# Store scalings in a dictionary so we can convert back later\n",
        "scaled_features = {}\n",
        "for each in quant_features:\n",
        "    mean, std = clean_df[each].mean(), clean_df[each].std()\n",
        "    scaled_features[each] = [mean, std]\n",
        "    clean_df.loc[:, each] = (clean_df[each] - mean)/std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJKaFbXfHOc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save data for approximately the last 21 days \n",
        "test_data = clean_df[-21*24:]\n",
        "\n",
        "# Now remove the test data from the data set \n",
        "clean_df = clean_df[:-21*24]\n",
        "\n",
        "# Separate the data into features and targets\n",
        "target_fields = ['cnt', 'casual', 'registered']\n",
        "features, targets = clean_df.drop(target_fields, axis=1), clean_df[target_fields]\n",
        "test_features, test_targets = test_data.drop(target_fields, axis=1), test_data[target_fields]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDhi6wbXR69h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_features, train_targets = features[:-60*24], targets[:-60*24]\n",
        "val_features, val_targets = features[-60*24:], targets[-60*24:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEjJgQi4TLFz",
        "colab_type": "text"
      },
      "source": [
        "## Building Neural Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwHMU1SAkILk",
        "colab_type": "text"
      },
      "source": [
        "Sigmoid activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3sjgPw9cLRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "  s = 1 / 1.0/(1.0+np.exp(-z))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAHfJtSzcKuP",
        "colab_type": "text"
      },
      "source": [
        "Relu activation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa0QS1gLky2Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(z):\n",
        "  return np.max(0, z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkHbWZW9cOHn",
        "colab_type": "text"
      },
      "source": [
        "Relu back propagation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d30M3kdW0TSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu_backward(dA, cache):\n",
        "  Z= cache\n",
        "  dZ = np.array(dA, copy=True)\n",
        "\n",
        "  dZ[Z <= 0] = 0\n",
        "\n",
        "  return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EMU1WJ8cRyl",
        "colab_type": "text"
      },
      "source": [
        "Sigmoid back propagation function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKKUYEVy0vlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid_backward(dA, cache):\n",
        "  Z = cache\n",
        "\n",
        "  s= 1/(1+np.exp(-Z))\n",
        "  dZ = dA * s * (1-s)\n",
        "\n",
        "  return dZ"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ_utvfkcWSY",
        "colab_type": "text"
      },
      "source": [
        "## Building Model\n",
        "\n",
        "The class performs the following activities\n",
        "\n",
        "1. Initialize parameters\n",
        "2.apply the sigmoid activation function\n",
        "3. Model training method \n",
        "4. model eval method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1MGvSQCNkJ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork(object):\n",
        "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
        "        # Set number of nodes in input, hidden and output layers.\n",
        "        self.input_nodes = input_nodes\n",
        "        self.hidden_nodes = hidden_nodes\n",
        "        self.output_nodes = output_nodes\n",
        "\n",
        "        # Initialize weights\n",
        "        self.weights_input_to_hidden = np.random.normal(0.0, self.input_nodes**-0.5, \n",
        "                                       (self.input_nodes, self.hidden_nodes))\n",
        "\n",
        "        self.weights_hidden_to_output = np.random.normal(0.0, self.hidden_nodes**-0.5, \n",
        "                                       (self.hidden_nodes, self.output_nodes))\n",
        "        self.lr = learning_rate\n",
        "        \n",
        "        # Note: in Python, you can define a function with a lambda expression,\n",
        "        self.activation_function = lambda x : 1.0/(1.0+np.exp(-x))  # Replace 0 with your sigmoid calculation.\n",
        "                    \n",
        "    \n",
        "    def train(self, features, targets):\n",
        "\n",
        "        n_records = features.shape[0]\n",
        "        delta_weights_i_h = np.zeros(self.weights_input_to_hidden.shape)\n",
        "        delta_weights_h_o = np.zeros(self.weights_hidden_to_output.shape)\n",
        "        for X, y in zip(features, targets):\n",
        "            #### Implement the forward pass here ####\n",
        "            ### Forward pass ###\n",
        "            hidden_inputs = np.dot(X, self.weights_input_to_hidden) # signals into hidden layer\n",
        "            hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
        "\n",
        "            final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
        "            final_outputs = final_inputs # signals from final output layer\n",
        "            \n",
        "            #### Implement the backward pass here ####\n",
        "            ### Backward pass ###\n",
        "\n",
        "            error = y - final_outputs  # Output layer error is the difference between desired target and actual output.\n",
        "            \n",
        "            hidden_error = np.dot(error, self.weights_hidden_to_output.T)\n",
        "\n",
        "            output_error_term = error\n",
        "            hidden_error_term = hidden_error * hidden_outputs * (1 - hidden_outputs)\n",
        "\n",
        "            # Weight step (input to hidden)\n",
        "            delta_weights_i_h += hidden_error_term * X[:, None] \n",
        "            # Weight step (hidden to output)\n",
        "            delta_weights_h_o += output_error_term * hidden_outputs[:, None]\n",
        "\n",
        "        self.weights_hidden_to_output += self.lr * delta_weights_h_o / n_records # update hidden-to-output weights with gradient descent step\n",
        "        self.weights_input_to_hidden += self.lr * delta_weights_i_h / n_records # update input-to-hidden weights with gradient descent step\n",
        " \n",
        "    def run(self, features):\n",
        "                \n",
        "        hidden_inputs = np.dot(features, self.weights_input_to_hidden) # signals into hidden layer\n",
        "        hidden_outputs = self.activation_function(hidden_inputs) # signals from hidden layer\n",
        "        \n",
        "        final_inputs = np.dot(hidden_outputs, self.weights_hidden_to_output) # signals into final output layer\n",
        "        final_outputs = final_inputs # signals from final output layer \n",
        "        \n",
        "        return final_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7PuiJFdc4Ub",
        "colab_type": "text"
      },
      "source": [
        "Calculate the mean square error"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIc2LnoAN74c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MSE(y, Y):\n",
        "    return np.mean((y-Y)**2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vI5hziGjc-d3",
        "colab_type": "text"
      },
      "source": [
        "## Training model\n",
        "\n",
        "1. Tuning hyper-parameters\n",
        "2. loading the model\n",
        "3. Training model on the training set\n",
        "4. evaluating the model on the validation set\n",
        "\n",
        "This ommits the training and validation losses\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgboG7mmN-OP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "### Set the hyperparameters here ###\n",
        "iterations = 1000\n",
        "learning_rate = 0.2\n",
        "hidden_nodes = 16\n",
        "output_nodes = 1\n",
        "\n",
        "N_i = train_features.shape[1]\n",
        "network = NeuralNetwork(N_i, hidden_nodes, output_nodes, learning_rate)\n",
        "\n",
        "losses = {'train':[], 'validation':[]}\n",
        "for ii in range(iterations):\n",
        "    # Go through a random batch of 128 records from the training data set\n",
        "    batch = np.random.choice(train_features.index, size=128)\n",
        "    X, y = train_features.iloc[batch].values, train_targets.iloc[batch]['cnt']\n",
        "                             \n",
        "    network.train(X, y)\n",
        "    \n",
        "    # Printing out the training progress\n",
        "    train_loss = MSE(network.run(train_features).T, train_targets['cnt'].values)\n",
        "    val_loss = MSE(network.run(val_features).T, val_targets['cnt'].values)\n",
        "    sys.stdout.write(\"\\rProgress: {:2.1f}\".format(100 * ii/float(iterations)) \\\n",
        "                     + \"% ... Training loss: \" + str(train_loss)[:5] \\\n",
        "                     + \" ... Validation loss: \" + str(val_loss)[:5])\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    losses['train'].append(train_loss)\n",
        "    losses['validation'].append(val_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ethlMEv_dVNI",
        "colab_type": "text"
      },
      "source": [
        "## Plotting the Train and Val Loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b2lmn5wSNZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(losses['train'], label='Training loss', color='red')\n",
        "plt.plot(losses['validation'], label='Validation loss', color='green')\n",
        "plt.legend()\n",
        "#_ = plt.ylim()\n",
        "plt.ylim(0.2, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3gWVM8-dc6o",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating the results graphically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U27SBZJLSVBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(8,4))\n",
        "\n",
        "mean, std = scaled_features['cnt']\n",
        "# Make Predictions on `test_features`\n",
        "predictions = network.run(test_features).T*std + mean\n",
        "ax.plot(predictions[0], label='Prediction', color='green')\n",
        "# Rescale data back to original shape\n",
        "ax.plot((test_targets['cnt']*std + mean).values, label='Data', color='orange')\n",
        "ax.set_xlim(right=len(predictions))\n",
        "ax.legend()\n",
        "\n",
        "dates = pd.to_datetime(df1.iloc[test_data.index]['dteday'])\n",
        "dates = dates.apply(lambda d: d.strftime('%b %d'))\n",
        "ax.set_xticks(np.arange(len(dates))[1::30])\n",
        "_ = ax.set_xticklabels(dates[1::30], rotation=45)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}